---
title: "Cleaning and analyzing data"
author: "Revati Shelat"
date: "5/13/2022"
output:
  word_document: default
  html_document: default
---

```{r}

#install.packages("tidytext")

if (!requireNamespace("textdata", quietly = TRUE)) {
  install.packages("textdata")
}
library(rtweet)
library(xlsx)
library(dplyr)
library(tidyr)
library(tidytext)
library(textdata)
library(ggplot2)
library(purrr) #for map function
```

```{r}

###Read Raw data files

path = ""

data <- read_excel() # add as required

```



```{r}

#Bind all tweets
#twtdata <- rbind(d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12, d13) 
#twtdata #show all metadata

twtdata <- data
tweets.twtdata <- twtdata%>%select(screen_name, text)
head(tweets.twtdata$text)

#Remove http elements
tweets.twtdata$stripped_text1 <- gsub("http\\S+", "", tweets.twtdata$text)
#Remove punctuation + ADDING id for each tweet
tweets.twtdata_stem <- tweets.twtdata %>%
  select(stripped_text1) %>%
  unnest_tokens(word, stripped_text1)

head(tweets.twtdata_stem)
tail(tweets.twtdata_stem)


#REMOVING "stop words" from list of words
cleaned_tweets.twtdata <- tweets.twtdata_stem %>%
  anti_join(stop_words)
  
head(cleaned_tweets.twtdata)

#TOP 10 words in #climatechange for Oct 31st

cleaned_tweets.twtdata %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y=n)) +
  geom_col() +
  xlab(NULL) + 
  coord_flip() +
  theme_classic() +
  labs(x= "Count",
       y= "Unique Words",
       title= "Unique word counts found in #climatechange tweets")

#testmerge <- rbind(d1, d2)
#View(testmerge)

###SENTIMENT ANALYSIS

sentiment<-cleaned_tweets.twtdata %>%
inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
sentiment

###-------VISUALIZATION------

sentiment %>%
  group_by(sentiment) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill=sentiment)) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~sentiment, scales = "free_y") +
  labs(title = "Tweets containing '#climatechange' (UNCCC COP26)", 
       y="Contribution to sentiment",
       x= NULL) +
  coord_flip() + theme_bw()

```

```{r}

###--------SCORE-----------

sent_func <- function(twt) {
  twttable = tibble(text = twt) %>%
    mutate(
      stripped_text = gsub("http//S+", "", text)
    ) %>%
    unnest_tokens(word, stripped_text) %>%
    anti_join(stop_words) %>% 
    inner_join(get_sentiments("bing")) %>%
    count(word, sentiment, sort = TRUE) %>%
    ungroup() %>%
    mutate(
      score = case_when(
        sentiment=='negative'~n*(-1),
        sentiment=='positive'~n*1
      )
    )
  sent_score<- case_when(
    nrow(twttable)==0~0,
    nrow(twttable)>0~sum(twttable$score)
  )
        
  zero_type <- case_when (
    nrow(twttable)==0~"Type 1", #Type 1: No words at all (i.e. zero = no)
    nrow(twttable)>0~"Type 2" #Type 2:  Sum of words = 0
  )
list(score = sent_score, type = zero_type, twt_tbl = twttable)
}

twt_sent <- lapply(twtdata$text, function(x){sent_func(x)}) #add sentiment score to tibble - makes code go brrrr
twt_sent #displays all tibbles

```


```{r}

#----------TIBBLE WITH SPECIFIED TYPE, SCORE, AND HASHTAG------------
climate_sent =  tibble(
    hashtag = '#climatechange',
    score = unlist(map(twt_sent, 'score')),
    type = unlist(map(twt_sent, 'type'))
  )

climate_sent

#---------HISTROGRAM----------
ggplot(climate_sent, aes(x=score, fill = hashtag)) + geom_histogram(bins = 15, alpha = 0.6) + facet_grid(~hashtag) + theme_bw()

```
```{r}

#install.packages("writexl")
library(writexl)
write_xlsx(twtdata,"C:/Users/revat/Desktop/-/Stats Year 3/Stat Sem 6/twt project/twtdata.xlsx")

```

```{r}

#install.packages("stringr")
library(stringr)
library(purrr)
twtList<-list("hello", "bye #climate", "#climate", "#blah", "revati #shelat", "this is a test #soemthing ", "#not a hashtag #climate.change boop","Di motivi per scendere in strada ce ne sarebbero tanti: #climatechange, #equitàsociale, #dirittiumani...

Come si è visto con gli #attivisti #FFF, che #Draghi ha ringraziato, i #gruppidipressione possono servire.

Ma il #Millenarismo 2.0 dei #novax è uno schiaffo all'intelligenza", "##########.my.!stuff?! ")

str_extract_all(twtList, '#(?:\\S*\\w+)+')

# map = call a function for each result of the proceeding line
map(str_extract_all(twtList, '#(?:\\S*\\w+)+'), function(x) str_replace(x, "^#+", "#"))

# unlist (remove '0' since it's "null" and unroll sub-lists + concatenate then in single 1D list)
unlist(map(str_extract_all(twtList, '#(?:\\S*\\w+)+'), function(x) str_replace(x, "^#+", "#"))) #gets rid of character(0)'s

# Now put it all together and store the value in hashtagList
hashtagList<-unlist(map(str_extract_all(twtList, '#(?:\\S*\\w+)+'), function(x) str_replace(x, "^#+", "#")))
hashtagList

# table = generate a frequency table of counts for each element in the list
hashtagFrequency<-table(hashtagList)
sort(hashtagFrequency, decreasing = TRUE)
unname(hashtagFrequency["#climate"]) #get frequency of particular hashtag


```

```{r}
labels(twtdata)

#Create list with each element being text from a single tweet 
tweetsDataList<-as.list(twtdata$text)
#length(tweetsDataList)

#Create list of all extracted hashtags
allHashtagsList<-unlist(map(str_extract_all(tweetsDataList, '#(?:\\S*\\w+)+'), function(x) tolower(str_replace(x, "^#+", "#"))))
allHashtagsList

#Get list of hashtags with frequency
allHashtagsFrequency<-table(allHashtagsList)
allHashtagsFrequency

#Sort list by (asc/dec) frequency

sortedAllHashtagsList <- sort(allHashtagsFrequency, decreasing = TRUE)

write_to = ""

write_xlsx(as.data.frame(sortedAllHashtagsList),write_to)

```

